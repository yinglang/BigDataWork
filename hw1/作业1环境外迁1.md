### 1. 从虚拟机复制文件到本机，安装HDFS依赖
```
Ubuntu 32bit 用户, 复制
/home/guset/work
/usr/local/java/hadoop-2.6.0-64bit.tar.gz
/usr/local/java/hbase-0.98.11-hadoop2-bin.tar.gz
/usr/local/java/jdk-7u40-linux-i586.tar.gz

Ubuntu 64bit 用户, 复制，
/home/guset/work
/usr/local/java/hadoop-2.6.0-64bit.tar.gz
/usr/local/java/hbase-0.98.11-hadoop2-bin.tar.gz
jdk需要自行下载: http://www.oracle.com/technetwork/java/javase/downloads/java-archive-downloads-javase7-521261.html 中的jdk-7u40-linux-x64.tar.gz
（jdk由于虚拟机上是linux 32位的所以无法复制只能重下）

windows 用户，复制，
/home/guset/work
/usr/local/java/hadoop-2.6.0-64bit.tar.gz
/usr/local/java/hbase-0.98.11-hadoop2-bin.tar.gz
jdk需要自行下载: http://www.oracle.com/technetwork/java/javase/downloads/java-archive-downloads-javase7-521261.html 中的jdk-7u40-linux-x64.tar.gz
（jdk由于虚拟机上是linux 32位的所以无法复制只能重下）
```

安装HDFS依赖库
```
$ sudo apt-get install ssh
$ sudo apt-get install rsync
```

### 2. 解压上述文件，配置环境变量
修改下面的JAVA_HOME， HADOOP_DIR， HBASE_DIR为自己解压的对应的目录
```bash
# add by hui for java
export JAVA_HOME="/home/hui/ide/jdk1.7.0_40"
export PATH="$JAVA_HOME/bin:$JAVA_HOME/jre/bin:$PATH"

# add by hui for hadoop
HADOOP_DIR=/home/hui/ide/hadoop/hadoop-2.9.0
HBASE_DIR=/home/hui/ide/hbase/hbase-1.2.6
# for compier .java file
export CLASSPATH=.:${HADOOP_DIR}/etc/hadoop:${HADOOP_DIR}/share/hadoop/common/lib/*:${HADOOP_DIR}/share/hadoop/common/*:${HADOOP_DIR}/share/hadoop/hdfs:${HADOOP_DIR}/share/hadoop/hdfs/lib/*:${HADOOP_DIR}/share/hadoop/hdfs/*:${HADOOP_DIR}/share/hadoop/yarn/lib/*:${HADOOP_DIR}/share/hadoop/yarn/*:${HADOOP_DIR}/share/hadoop/mapreduce/lib/*:${HADOOP_DIR}/share/hadoop/mapreduce/*:${HBASE_DIR}/lib/*
export PATH=${PATH}:${HADOOP_DIR}/bin:${HADOOP_DIR}/sbin:${HBASE_DIR}/bin
# for run .class file
export LD_LIBRARY_PATH=${HADOOP_DIR}/lib/native
```

### 3. hadoop的相关配置
在${HADOOP_DIR}/etc/hadoop/core-site.xml中增加如下配置：<br/>
修改hadoop.tmp.dir的value为自己复制的work的目录
```
<configuration>
  <property>
      <name>fs.defaultFS</name>
      <value>hdfs://localhost:9000</value>
  </property>
  <property>
      <name>hadoop.tmp.dir</name>
      <value>/home/hui/work/hdfs</value>
  </property>
</configuration>
```

修改/etc/hadoop/hadoop-env.sh中设JAVA_HOME。
```
export JAVA_HOME=/home/hui/ide/jdk1.7.0_45        //正确，应该这么改
```

### 4. 配置免密码登录
如果我们需要免密码ssh连接到hadoop，只需要把本机器的公钥(id_rsa.pub)追加到目标机器（单机就是本机）的authorized_keys下
```
cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
```

### 5. 测试环境安装的正确性
```bash
hui@hui-XPS-8920:~$ java -version
java version "1.7.0_45"
Java(TM) SE Runtime Environment (build 1.7.0_45-b18)
Java HotSpot(TM) 64-Bit Server VM (build 24.45-b08, mixed mode)

hui@hui-XPS-8920:~$ start-dfs.sh
Starting namenodes on [localhost]
localhost: starting namenode, logging to /home/hui/ide/hadoop/hadoop-2.6.5/logs/hadoop-hui-namenode-hui-XPS-8920.out
localhost: starting datanode, logging to /home/hui/ide/hadoop/hadoop-2.6.5/logs/hadoop-hui-datanode-hui-XPS-8920.out
Starting secondary namenodes [0.0.0.0]
0.0.0.0: starting secondarynamenode, logging to /home/hui/ide/hadoop/hadoop-2.6.5/logs/hadoop-hui-secondarynamenode-hui-XPS-8920.out

hui@hui-XPS-8920:~$ telnet 127.0.0.1 9000  # Ctrl + c, Enter退出
Trying 127.0.0.1...
Connected to 127.0.0.1.
Escape character is '^]'.
^C
Connection closed by foreign host.

hui@hui-XPS-8920:~$ stop-dfs.sh
Stopping namenodes on [localhost]
localhost: stopping namenode
localhost: stopping datanode
Stopping secondary namenodes [0.0.0.0]
0.0.0.0: stopping secondarynamenode

hui@hui-XPS-8920:~$ start-hbase.sh
starting master, logging to /home/hui/ide/hbase/hbase-1.2.6/bin/../logs/hbase-hui-master-hui-XPS-8920.out
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/hui/ide/hbase/hbase-1.2.6/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/hui/ide/hadoop/hadoop-2.6.5/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]

hui@hui-XPS-8920:~$ stop-hbase.sh
stopping hbase..........................................................
```

运行work下的代码, 如果没有报错说明环境外迁成功完成
```bash
cd /home/hui/work/hw1  # 这里请切换到你自己复制的work目录下

start-dfs.sh
javac HDFSTest.java
java HDFSTest

start-hbase.sh
javac HBaseTest.java
java HBaseTest
stop-hbase.sh

stop-dfs.sh
```
